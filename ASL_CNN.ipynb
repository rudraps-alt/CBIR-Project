{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faccfa03-0338-454b-90b4-f620e42f6afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\shakt\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:111: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from random import shuffle\n",
    "import tflearn\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f54d52a3-16e2-4f26-b34d-a84a7135de5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter the image preocesing model to use 1)ycbcr 2)edge detection using filters 3)HSV 4)hsv with histogram back projection :  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ycbcr\n",
      "contain folder in same name\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE=96\n",
    "\n",
    "# region of interest (ROI) coordinates\n",
    "top, right, bottom, left = 100, 150, 400, 450\n",
    "\n",
    "a=''\n",
    "arr = ['ycbcr','edge detection using filters', 'HSV' , 'hsv with histogram back projection']\n",
    "color_model=int(input('enter the image preocesing model to use 1)ycbcr 2)edge detection using filters 3)HSV 4)hsv with histogram back projection : '))\n",
    "\n",
    "dir0 = arr[color_model-1]\n",
    "print(dir0)\n",
    "try:\n",
    "    os.mkdir(dir0)\n",
    "except:\n",
    "    print('contain folder in same name')\n",
    "\n",
    "# get the reference to the webcam\n",
    "camera = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c822ef6-ac89-4673-9fbb-39602ed29901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve(B, r):\n",
    "    D = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(r,r))\n",
    "    cv2.filter2D(B, -1, D, B)\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c45e4ab8-cfce-4171-8132-2ed52dc348db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def img_process(image_frame,color_model):        \n",
    "    if color_model==1:\n",
    "        gray = cv2.cvtColor(image_frame, cv2.COLOR_BGR2YCR_CB)\n",
    "        #gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        skin_ycrcb_min = np.array((0, 133, 77))\n",
    "        skin_ycrcb_max = np.array((255, 173, 127))\n",
    "        gray = cv2.inRange(gray, skin_ycrcb_min, skin_ycrcb_max);  # detecting the hand in the bounding box using skin detection \n",
    "        contours,hierarchy = cv2.findContours(gray.copy(),cv2.RETR_EXTERNAL, 2)\n",
    "        #resize img\n",
    "        gray = cv2.GaussianBlur(gray, (7, 7), 0)\n",
    "        ret, gray = cv2.threshold(gray, 120, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        gray = cv2.resize(gray, (IMG_SIZE,IMG_SIZE))\n",
    "        \n",
    "    elif color_model==2:\n",
    "        gray = cv2.cvtColor(image_frame, cv2.COLOR_BGR2GRAY)\n",
    "        blur = cv2.GaussianBlur(gray,(5,5),2)\n",
    "        th3 = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY_INV,11,2)\n",
    "        ret, gray = cv2.threshold(th3, minValue, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "        gray = cv2.resize(gray, (IMG_SIZE,IMG_SIZE))\n",
    "    \n",
    "    elif color_model==3:\n",
    "        min_HSV = np.array([0, 58, 30], dtype = \"uint8\")\n",
    "        max_HSV = np.array([33, 255, 255], dtype = \"uint8\")\n",
    "        # Get pointer to video frames from primary device\n",
    "        image = image_frame\n",
    "        imageHSV = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "        skinRegionHSV = cv2.inRange(imageHSV, min_HSV, max_HSV)\n",
    "        skinHSV = cv2.bitwise_and(image, image, mask = skinRegionHSV)\n",
    "        gray = np.hstack([image, skinHSV])\n",
    "        gray = cv2.GaussianBlur(gray, (7, 7), 0)\n",
    "        ret, gray = cv2.threshold(gray, 120, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        gray = cv2.resize(gray, (IMG_SIZE,IMG_SIZE))\n",
    "        \n",
    "    elif color_model==4:\n",
    "        image_hsv = cv2.cvtColor(image_frame,cv2.COLOR_BGR2HSV)\n",
    "        M = cv2.calcHist([image_hsv], channels=[0, 1], mask=None, histSize=[80, 256], ranges=[0, 180, 0, 256] )\n",
    "        B = convolve(M, r=5)\n",
    "        ret, gray = cv2.threshold(th3, 30, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "    return gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec69a4f-95e3-4d0d-bd78-1b37b3f663f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "exit: e or enter the label name :  e\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "\n",
    "    a=input('exit: e or enter the label name : ')\n",
    "\n",
    "    if a==\"e\":\n",
    "        break\n",
    "\n",
    "    dir1=str(dir0)+'/'+str(a)\n",
    "    print(dir1)\n",
    "\n",
    "    try:\n",
    "        os.mkdir(dir1)\n",
    "    except:\n",
    "        print('contain folder')\n",
    "\n",
    "    i=0\n",
    "\n",
    "    while(True):\n",
    "        (t, frame) = camera.read()\n",
    "\n",
    "        \n",
    "\n",
    "        # flip the frame so that it is not the mirror view\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # get the ROI\n",
    "        roi = frame[top:bottom, right:left]\n",
    "\n",
    "        gray = img_process(roi,color_model)\n",
    "    \n",
    "        #write img file to directory\n",
    "        cv2.imwrite(\"%s/%s/%d.jpg\"%(dir0,a,i),gray)\n",
    "        i+=1\n",
    "        print(i)\n",
    "        if i>500:\n",
    "            break\n",
    "\n",
    "        # draw the segmented hand\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0,255,0), 2)\n",
    "\n",
    "        cv2.imshow(\"Video Feed 1\", gray)\n",
    "\n",
    "        cv2.imshow(\"Video Feed\", frame)\n",
    "        # observe the keypress by the user\n",
    "        keypress = cv2.waitKey(1)\n",
    "\n",
    "        # if the user pressed \"Esc\", then stop looping\n",
    "        if keypress == 27:\n",
    "            break\n",
    "\n",
    "# free up memory\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4851034-408f-4212-8c08-abe495ac302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=dir0\n",
    "IMG_SIZE = 96\n",
    "def create_train_data():\n",
    "    training_data = []\n",
    "    label=0\n",
    "    for (dirpath,dirnames,filenames) in os.walk(path):\n",
    "        for dirname in dirnames:\n",
    "            print(dirname)\n",
    "            for(direcpath,direcnames,files) in os.walk(path+\"/\"+dirname):\n",
    "                for file in files:\n",
    "                        actual_path=path+\"/\"+dirname+\"/\"+file\n",
    "                        #print(files)\n",
    "                        # label=label_img(dirname)\n",
    "                        path1 =path+\"/\"+dirname+'/'+file\n",
    "                        img=cv2.imread(path1,cv2.IMREAD_GRAYSCALE)\n",
    "                        img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "                        training_data.append([np.array(img),label])\n",
    "            label=label+1\n",
    "            print(label)\n",
    "    shuffle(training_data)\n",
    "    np.save('train_data.npy', training_data)\n",
    "    print(training_data)\n",
    "    #return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b75ebc5d-62f0-41e7-ae3e-3281b6bb5ce8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "1\n",
      "B\n",
      "2\n",
      "C\n",
      "3\n",
      "D\n",
      "4\n",
      "E\n",
      "5\n",
      "F\n",
      "6\n",
      "G\n",
      "7\n",
      "H\n",
      "8\n",
      "I\n",
      "9\n",
      "J\n",
      "10\n",
      "K\n",
      "11\n",
      "L\n",
      "12\n",
      "M\n",
      "13\n",
      "N\n",
      "14\n",
      "O\n",
      "15\n",
      "P\n",
      "16\n",
      "Q\n",
      "17\n",
      "R\n",
      "18\n",
      "S\n",
      "19\n",
      "T\n",
      "20\n",
      "U\n",
      "21\n",
      "V\n",
      "22\n",
      "W\n",
      "23\n",
      "X\n",
      "24\n",
      "Y\n",
      "25\n",
      "Z\n",
      "26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shakt\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\numpy\\core\\_asarray.py:171: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n",
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa695e06-4325-444c-8991-0d2ddcf4ccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 96\n",
    "LR = 1e-3  #.001 learing rate\n",
    "\n",
    "nb_classes=28\n",
    "\n",
    "MODEL_NAME = 'handsign.model'\n",
    "\n",
    "def one_hot_targets_(labels_dense,nb_classes):\n",
    "    targets = np.array(Y).reshape(-1)\n",
    "    print(targets)\n",
    "    one_hot_targets = np.eye(nb_classes)[targets]\n",
    "    return one_hot_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ecb2f78-dd12-4f30-9620-06f1e11a22f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traindatlen:13014\n",
      "testdatalen:100\n",
      "[ 0  5 17 ...  9  0 10]\n"
     ]
    }
   ],
   "source": [
    "train_data = np.load('train_data.npy',encoding=\"latin1\",allow_pickle=True)\n",
    "# test_data = np.load('test_data.npy',encoding=\"latin1\")\n",
    "\n",
    "train = train_data[:]\n",
    "test = train_data[:100]\n",
    "\n",
    "print('traindatlen:'+str(len(train)))\n",
    "print('testdatalen:'+str(len(test)))\n",
    "\n",
    "X = np.array([i[0] for i in train]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "Y = [i[1] for i in train]\n",
    "Y1=one_hot_targets_(Y,nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2709b2e-0c42-4421-bb44-7b18fe6fe180",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 96\n",
    "LR = 1e-3\n",
    "\n",
    "nb_classes=28\n",
    "\n",
    "MODEL_NAME = 'handsign.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e877d352-46d1-4cec-9357-3fc54088b122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model():\n",
    "    convnet = input_data(shape=[None, IMG_SIZE, IMG_SIZE, 1], name='input')\n",
    "    convnet = conv_2d(convnet, 8, 5, activation='relu')\n",
    "    convnet = max_pool_2d(convnet, 5)\n",
    "    convnet = conv_2d(convnet, 16, 5, activation='relu')\n",
    "    convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "    convnet = conv_2d(convnet, 32, 5, activation='relu')\n",
    "    convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "\n",
    "    convnet = conv_2d(convnet, 64, 5, activation='relu')\n",
    "    convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "    convnet = conv_2d(convnet, 128, 5, activation='relu')\n",
    "    convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "    convnet = fully_connected(convnet, 1024, activation='relu')\n",
    "    convnet = dropout(convnet, 0.8)\n",
    "    \n",
    "    convnet = fully_connected(convnet, nb_classes, activation='softmax')\n",
    "    convnet = regression(convnet, optimizer='adam', learning_rate=0.01, loss='categorical_crossentropy', name='targets')\n",
    "    \n",
    "    model = tflearn.DNN(convnet, tensorboard_dir='log')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a56ae02-cb0d-4db1-8448-c8b0b1af011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 96\n",
    "LR = 1e-3  #.001 learing rate\n",
    "\n",
    "nb_classes=28\n",
    "\n",
    "MODEL_NAME = 'handsign.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a25e8f35-b185-46f7-9746-c4e3e7749a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_targets_(labels_dense,nb_classes):\n",
    "    targets = np.array(Y).reshape(-1)\n",
    "    print(targets)\n",
    "    one_hot_targets = np.eye(nb_classes)[targets]\n",
    "    return one_hot_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68587d44-94b6-4fc4-896b-5d59af6a9e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traindatlen:13014\n",
      "testdatalen:100\n",
      "[ 0  5 17 ...  9  0 10]\n",
      "val y[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "len X:13014\n",
      "len Y:13014\n",
      "[ 0  5 17 ...  9  0 10]\n",
      "test_x:100\n",
      "test_y:13014\n",
      "val y[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "train_data = np.load('train_data.npy',encoding=\"latin1\",allow_pickle=True)\n",
    "# test_data = np.load('test_data.npy',encoding=\"latin1\")\n",
    "\n",
    "train = train_data[:]\n",
    "test = train_data[:100]\n",
    "\n",
    "print('traindatlen:'+str(len(train)))\n",
    "print('testdatalen:'+str(len(test)))\n",
    "\n",
    "X = np.array([i[0] for i in train]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "Y = [i[1] for i in train]\n",
    "Y1=one_hot_targets_(Y,nb_classes)\n",
    "# \n",
    "# print('max y'+str(max(Y)))\n",
    "# print('min y'+str(min(Y)))\n",
    "print('val y'+str(Y1))\n",
    "print('len X:'+str(len(X)))\n",
    "print('len Y:'+str(len(Y)))\n",
    "test_x = np.array([i[0] for i in test]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "test_y = [i[1] for i in test]\n",
    "test_y1=one_hot_targets_(test_y,nb_classes)\n",
    "test_y=test_y1\n",
    "Y=Y1\n",
    "print('test_x:'+str(len(test_x)))\n",
    "print('test_y:'+str(len(test_y)))\n",
    "print('val y'+str(test_y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ccc3f1d-be3c-471c-a707-1cbcfdd62097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3059  | total loss: \u001b[1m\u001b[32m1.14144\u001b[0m\u001b[0m | time: 22.784s\n",
      "| Adam | epoch: 015 | loss: 1.14144 - acc: 0.8701 -- iter: 12992/13014\n",
      "Training Step: 3060  | total loss: \u001b[1m\u001b[32m1.06669\u001b[0m\u001b[0m | time: 24.045s\n",
      "| Adam | epoch: 015 | loss: 1.06669 - acc: 0.8721 | val_loss: 0.75527 - val_acc: 0.9000 -- iter: 13014/13014\n",
      "--\n",
      "INFO:tensorflow:C:\\Users\\shakt\\handsign.model is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "model=cnn_model()\n",
    "\n",
    "model.fit({'input': X}, {'targets': Y}, n_epoch=15, validation_set=({'input': test_x}, {'targets': test_y}), \n",
    "snapshot_step=500, show_metric=True, run_id=MODEL_NAME)\n",
    "\n",
    "model.save(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0e3dd7f-49d8-4b83-a7c9-871a6cbd7901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuarcy: 90.00%\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_x, test_y)\n",
    "print('Test accuarcy: %0.2f%%' % (score[0] * 100))\n",
    "print(color_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97628353-2ecf-4762-b3e9-308b41c3fe68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\shakt\\handsign.model\n",
      "model loaded!\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('{}.meta'.format(MODEL_NAME)):\n",
    "    model.load(MODEL_NAME)\n",
    "    print('model loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92593ae2-1f27-445f-ba8a-d8efdb2766b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_label=['U', 'T', 'E', 'R', 'BKSP', 'Q', 'D', 'BLNK1', 'G', 'SPC', 'I', 'F', 'O', 'C', 'W', 'Y', 'N', 'V', 'H', 'H', 'P', 'A', 'S', 'L', 'K', 'X', 'BLNK', 'B', ]\n",
    "pre=[]\n",
    "\n",
    "s=''\n",
    "cchar=[0,0]\n",
    "c1=''\n",
    "\n",
    "# initialize weight for running average\n",
    "aWeight = 0.5\n",
    "\n",
    "# get the reference to the webcam\n",
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "# region of interest (ROI) coordinates\n",
    "top, right, bottom, left = 170, 150, 425, 450\n",
    "\n",
    "# initialize num of frames\n",
    "num_frames = 0\n",
    "\n",
    "flag=0\n",
    "flag1=0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
